{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aBgS5yu0sIhc","outputId":"36e2a0e2-c6a5-49a0-9204-262a002619d8","executionInfo":{"status":"ok","timestamp":1668818137564,"user_tz":480,"elapsed":43980,"user":{"displayName":"Swaptik Chowdhury","userId":"10959842892116898495"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]},{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting biterm\n","  Downloading biterm-0.1.5.tar.gz (79 kB)\n","\u001b[K     |████████████████████████████████| 79 kB 3.7 MB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from biterm) (1.21.6)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from biterm) (4.64.1)\n","Requirement already satisfied: cython in /usr/local/lib/python3.7/dist-packages (from biterm) (0.29.32)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from biterm) (3.7)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk->biterm) (7.1.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.7/dist-packages (from nltk->biterm) (2022.6.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk->biterm) (1.2.0)\n","Building wheels for collected packages: biterm\n","  Building wheel for biterm (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for biterm: filename=biterm-0.1.5-cp37-cp37m-linux_x86_64.whl size=195882 sha256=3876e69dc2b5269e3cc90d8f445dde96896c3eac3a71f500cf288b12354546dd\n","  Stored in directory: /root/.cache/pip/wheels/35/45/73/c4a79327f13728b195c12f35aa9331f897e37786f8e446cb09\n","Successfully built biterm\n","Installing collected packages: biterm\n","Successfully installed biterm-0.1.5\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting pyLDAvis\n","  Downloading pyLDAvis-3.3.1.tar.gz (1.7 MB)\n","\u001b[K     |████████████████████████████████| 1.7 MB 9.2 MB/s \n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n","    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n","Collecting funcy\n","  Downloading funcy-1.17-py2.py3-none-any.whl (33 kB)\n","Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (1.21.6)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (57.4.0)\n","Collecting sklearn\n","  Downloading sklearn-0.0.post1.tar.gz (3.6 kB)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (1.2.0)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (2.11.3)\n","Requirement already satisfied: pandas>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (1.3.5)\n","Requirement already satisfied: numexpr in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (2.8.4)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (1.0.2)\n","Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (3.6.0)\n","Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (0.16.0)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (1.7.3)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.2.0->pyLDAvis) (2.8.2)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.2.0->pyLDAvis) (2022.6)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=1.2.0->pyLDAvis) (1.15.0)\n","Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim->pyLDAvis) (5.2.1)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->pyLDAvis) (2.0.1)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->pyLDAvis) (3.1.0)\n","Building wheels for collected packages: pyLDAvis, sklearn\n","  Building wheel for pyLDAvis (PEP 517) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pyLDAvis: filename=pyLDAvis-3.3.1-py2.py3-none-any.whl size=136898 sha256=a0b64223da69aeef2ff785c52e404196c6d46b3c520d21bd4dc4c192bb6fac16\n","  Stored in directory: /root/.cache/pip/wheels/c9/21/f6/17bcf2667e8a68532ba2fbf6d5c72fdf4c7f7d9abfa4852d2f\n","  Building wheel for sklearn (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sklearn: filename=sklearn-0.0.post1-py3-none-any.whl size=2344 sha256=8e94b1a474c2cea375499834ae4fa7bc7d98e8c84e0cda25edf16d1573e75323\n","  Stored in directory: /root/.cache/pip/wheels/42/56/cc/4a8bf86613aafd5b7f1b310477667c1fca5c51c3ae4124a003\n","Successfully built pyLDAvis sklearn\n","Installing collected packages: sklearn, funcy, pyLDAvis\n","Successfully installed funcy-1.17 pyLDAvis-3.3.1 sklearn-0.0.post1\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/past/types/oldstr.py:5: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n","  from collections import Iterable\n","/usr/local/lib/python3.7/dist-packages/past/builtins/misc.py:4: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n","  from collections import Mapping\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":1}],"source":["#https://github.com/markoarnauto/biterm\n","import pandas as pd\n","import numpy as np\n","import os\n","import re\n","from nltk.tokenize import word_tokenize\n","from nltk.stem import PorterStemmer\n","import scipy\n","import nltk\n","nltk.download('stopwords')\n","!pip install biterm\n","!pip install pyLDAvis\n","import pyLDAvis\n","from biterm.btm import oBTM \n","from sklearn.feature_extraction.text import CountVectorizer\n","from biterm.utility import vec_to_biterms, topic_summuary \n","nltk.download('punkt') # needed for apply function "]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Weq1Ud1_sunV","outputId":"970b36b7-254a-49fc-b4f1-b4eddd2b0e60","executionInfo":{"status":"ok","timestamp":1668818338528,"user_tz":480,"elapsed":20295,"user":{"displayName":"Swaptik Chowdhury","userId":"10959842892116898495"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["os.getcwd() # to see working directory\n","os.chdir(\"/content/drive/MyDrive/Web 3.0\") #changing workig directory to google drive"],"metadata":{"id":"Tilmg3QFt4wE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df=pd.read_excel(\"Downloaded_9-12_Web3 _7 days.xlsx\")\n","#df.columns # to check number of column\n","df_originalt = df[df[\"Engagement Type\"].isnull()] # Only looking at original tweets \n","df_fulltext =df_originalt[[\"Full Text\"]] # Then selecting the original texts \n"],"metadata":{"id":"QhbtHHoZuKOT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def remove_content(text):\n","    text = re.sub(r'(https|http)?:\\/\\/(\\w|\\.|\\/|\\?|\\=|\\&|\\%)*\\b', '', text, flags=re.MULTILINE) # To Remove http, https and the other normal url type special characters\n","    text=re.sub(r'\\S+\\.com\\S+','',text) #remove urls.\"\" mean no space or nothing\n","    text=re.sub(r'\\@\\w+','',text) #remove mentions\n","    text =re.sub(r'\\#\\w+','',text) #remove hashtags\n","    text =re.sub(r\"^RT\\s+\",'',text) #remove rt's. re.sub() used to replace substrings; Syntax: re.sub(pattern,replacement,string) & will replace the matches in string with repl.\n","    return text"],"metadata":{"id":"QVq2Gl5a3DXa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from nltk.stem.snowball import stopwords\n","def process_text(text, stem= False): #clean text\n","    text=remove_content(text)\n","    text = re.sub('[^A-Za-z0-9]', ' ', text.lower()) #remove non-alphabets(?); lower is method applied to string text; matching each word in text-replacing with lower version\n","    tokenized_text = word_tokenize(text) #tokenize\n","    clean_text = [\n","         word for word in tokenized_text\n","         #if word not in stopwords.word(\"english\")\n","         if word not in stopwords.words(\"english\")\n","    ]\n","    if stem:\n","        stemmer = PorterStemmer() # initializig a poterstemmer object\n","        clean_text=[stemmer.stem(word) for word in clean_text]\n","    return ' '.join(clean_text) \n","\n","# String.join(iterable).The join() method takes all items in an iterable and joins them into one string.\n","# Example: let myTuple = (\"John\", \"Peter\", \"Vicky\"), x = \"#\".join(myTuple), print(x),  John#Peter#Vicky"],"metadata":{"id":"v9o2-vga9E3H"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_fulltext[\"Full Text\"]=df_fulltext[\"Full Text\"].astype(\"str\") # In the initial dataset not all the values were string. So this convert every value to string (shown as object type)\n","df_reduced = df_fulltext.sample(frac=1) # for testing the analytical pipeline, use smaller sample size \n","df_reduced['cleaned_tweets']=df_reduced[\"Full Text\"].apply(lambda x: process_text(x))\n","df_reduced[\"tokenized_cleantweets\"] = df_reduced[\"cleaned_tweets\"].apply(lambda x:word_tokenize(x))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"W2ulmZwU-IuU","outputId":"276f41d9-e97e-4d6c-98a7-0cfdd98bdc8d","executionInfo":{"status":"ok","timestamp":1668392136400,"user_tz":480,"elapsed":30216,"user":{"displayName":"Swaptik Chowdhury","userId":"10959842892116898495"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  \"\"\"Entry point for launching an IPython kernel.\n"]}]},{"cell_type":"code","source":["#pyLDAvis has issues with documents with very few tokens. So here we are filtering for tweets with atleast 3 tokens in them.\n","i=0\n","pylda = []\n","while i<len(df_reduced ):\n","  #print(i)\n","  if len (df_reduced.iloc[i,2]) > 5:\n","    A = df_reduced.iloc[i,2]\n","  else:\n","    A= 0\n","  pylda.append(A)\n","  i+=1\n","  pylda = [i for i in pylda if i!=0]\n","df_a = pd.DataFrame()\n","df_a [\"Clean\"] = pylda # this is a df containing full text with token >3. Look for method to reduce creation of df's"],"metadata":{"id":"wBAhVOXZivLn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_a[\"Clean\"]=df_a[\"Clean\"].astype(\"str\") # str is a req for vectorizer\n","#docs_r = df_r['cleaned_tweets'].tolist() # Converting inviduals tweets from excel to list\n","docs_r = df_a[\"Clean\"].tolist()"],"metadata":{"id":"FXcyS5gBrRl2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#vec = CountVectorizer(stop_words='english')\n","#X = vec.fit_transform(docs_r).toarray()\n","vec = CountVectorizer(stop_words=\"english\")\n","X = vec.fit_transform(docs_r).toarray() # Sparse Matrix which is being converted to array. So going from lean to dense."],"metadata":{"id":"IONXZ7T4Nw-0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["vocab = np.array(vec.get_feature_names())\n","biterms = vec_to_biterms(X)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-FHGfw8NHEGH","outputId":"9a3b42d9-a42e-4e31-d047-cab5e6eec918","executionInfo":{"status":"ok","timestamp":1668392203392,"user_tz":480,"elapsed":1457,"user":{"displayName":"Swaptik Chowdhury","userId":"10959842892116898495"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n","  warnings.warn(msg, category=FutureWarning)\n"]}]},{"cell_type":"code","source":["import multiprocessing as mp\n","mp.cpu_count() # to see how many cores I have"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3CbKLAzTi1y8","outputId":"8d653cba-665e-43df-8505-1c302df48e4d","executionInfo":{"status":"ok","timestamp":1668392214842,"user_tz":480,"elapsed":310,"user":{"displayName":"Swaptik Chowdhury","userId":"10959842892116898495"}}},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["2"]},"metadata":{},"execution_count":14}]},{"cell_type":"code","source":["btm = oBTM(num_topics=5, V=vocab)\n","for i in range(0, len(biterms), 100): \n","  biterms_chunk = biterms[i:i + 100]\n","  btm.fit(biterms_chunk, iterations=10)\n","topics = btm.transform(biterms)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Yp9Y220GLNVK","outputId":"04f7a421-4d6e-42fe-b15d-814a3fdec4fc","executionInfo":{"status":"ok","timestamp":1668397931165,"user_tz":480,"elapsed":5710892,"user":{"displayName":"Swaptik Chowdhury","userId":"10959842892116898495"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 10/10 [01:21<00:00,  8.13s/it]\n","100%|██████████| 10/10 [01:14<00:00,  7.50s/it]\n","100%|██████████| 10/10 [01:10<00:00,  7.02s/it]\n","100%|██████████| 10/10 [01:10<00:00,  7.08s/it]\n","100%|██████████| 10/10 [01:09<00:00,  6.95s/it]\n","100%|██████████| 10/10 [01:07<00:00,  6.75s/it]\n","100%|██████████| 10/10 [00:54<00:00,  5.40s/it]\n","100%|██████████| 10/10 [00:56<00:00,  5.61s/it]\n","100%|██████████| 10/10 [01:06<00:00,  6.68s/it]\n","100%|██████████| 10/10 [01:09<00:00,  6.96s/it]\n","100%|██████████| 10/10 [01:00<00:00,  6.02s/it]\n","100%|██████████| 10/10 [01:12<00:00,  7.29s/it]\n","100%|██████████| 10/10 [01:14<00:00,  7.46s/it]\n","100%|██████████| 10/10 [01:06<00:00,  6.60s/it]\n","100%|██████████| 10/10 [00:59<00:00,  5.91s/it]\n","100%|██████████| 10/10 [01:11<00:00,  7.17s/it]\n","100%|██████████| 10/10 [00:58<00:00,  5.82s/it]\n","100%|██████████| 10/10 [01:02<00:00,  6.28s/it]\n","100%|██████████| 10/10 [01:08<00:00,  6.89s/it]\n","100%|██████████| 10/10 [00:56<00:00,  5.69s/it]\n","100%|██████████| 10/10 [01:09<00:00,  6.97s/it]\n","100%|██████████| 10/10 [01:08<00:00,  6.88s/it]\n","100%|██████████| 10/10 [01:06<00:00,  6.69s/it]\n","100%|██████████| 10/10 [00:57<00:00,  5.72s/it]\n","100%|██████████| 10/10 [01:03<00:00,  6.33s/it]\n","100%|██████████| 10/10 [01:18<00:00,  7.89s/it]\n","100%|██████████| 10/10 [01:11<00:00,  7.18s/it]\n","100%|██████████| 10/10 [01:05<00:00,  6.55s/it]\n","100%|██████████| 10/10 [01:08<00:00,  6.89s/it]\n","100%|██████████| 10/10 [01:07<00:00,  6.71s/it]\n","100%|██████████| 10/10 [01:14<00:00,  7.49s/it]\n","100%|██████████| 10/10 [01:15<00:00,  7.52s/it]\n","100%|██████████| 10/10 [01:05<00:00,  6.60s/it]\n","100%|██████████| 10/10 [01:17<00:00,  7.72s/it]\n","100%|██████████| 10/10 [01:10<00:00,  7.02s/it]\n","100%|██████████| 10/10 [01:10<00:00,  7.09s/it]\n","100%|██████████| 10/10 [01:10<00:00,  7.09s/it]\n","100%|██████████| 10/10 [01:08<00:00,  6.85s/it]\n","100%|██████████| 10/10 [01:08<00:00,  6.86s/it]\n","100%|██████████| 10/10 [01:08<00:00,  6.87s/it]\n","100%|██████████| 10/10 [00:59<00:00,  5.91s/it]\n","100%|██████████| 10/10 [01:06<00:00,  6.65s/it]\n","100%|██████████| 10/10 [01:07<00:00,  6.78s/it]\n","100%|██████████| 10/10 [00:55<00:00,  5.55s/it]\n","100%|██████████| 10/10 [01:01<00:00,  6.13s/it]\n","100%|██████████| 10/10 [01:06<00:00,  6.67s/it]\n","100%|██████████| 10/10 [01:07<00:00,  6.73s/it]\n","100%|██████████| 10/10 [01:14<00:00,  7.48s/it]\n","100%|██████████| 10/10 [01:10<00:00,  7.03s/it]\n","100%|██████████| 10/10 [01:11<00:00,  7.12s/it]\n","100%|██████████| 10/10 [01:06<00:00,  6.65s/it]\n","100%|██████████| 10/10 [01:06<00:00,  6.62s/it]\n","100%|██████████| 10/10 [01:06<00:00,  6.64s/it]\n","100%|██████████| 10/10 [01:08<00:00,  6.84s/it]\n","100%|██████████| 10/10 [01:20<00:00,  8.05s/it]\n","100%|██████████| 10/10 [01:07<00:00,  6.73s/it]\n","100%|██████████| 10/10 [01:01<00:00,  6.15s/it]\n","100%|██████████| 10/10 [01:06<00:00,  6.62s/it]\n","100%|██████████| 10/10 [01:12<00:00,  7.22s/it]\n","100%|██████████| 10/10 [01:11<00:00,  7.16s/it]\n","100%|██████████| 10/10 [01:07<00:00,  6.80s/it]\n","100%|██████████| 10/10 [01:11<00:00,  7.11s/it]\n","100%|██████████| 10/10 [01:02<00:00,  6.25s/it]\n","100%|██████████| 10/10 [01:22<00:00,  8.23s/it]\n","100%|██████████| 10/10 [01:14<00:00,  7.43s/it]\n","100%|██████████| 10/10 [01:01<00:00,  6.16s/it]\n","100%|██████████| 10/10 [01:05<00:00,  6.59s/it]\n","100%|██████████| 10/10 [01:08<00:00,  6.82s/it]\n","100%|██████████| 10/10 [01:06<00:00,  6.63s/it]\n","100%|██████████| 10/10 [01:03<00:00,  6.40s/it]\n","100%|██████████| 10/10 [01:14<00:00,  7.42s/it]\n","100%|██████████| 10/10 [01:11<00:00,  7.13s/it]\n","100%|██████████| 10/10 [01:19<00:00,  7.98s/it]\n","100%|██████████| 10/10 [01:03<00:00,  6.36s/it]\n","100%|██████████| 10/10 [01:05<00:00,  6.50s/it]\n","100%|██████████| 10/10 [01:14<00:00,  7.42s/it]\n","100%|██████████| 10/10 [01:18<00:00,  7.87s/it]\n","100%|██████████| 10/10 [01:04<00:00,  6.50s/it]\n","100%|██████████| 10/10 [01:03<00:00,  6.32s/it]\n","100%|██████████| 10/10 [00:59<00:00,  5.96s/it]\n","100%|██████████| 10/10 [01:07<00:00,  6.79s/it]\n","100%|██████████| 10/10 [01:08<00:00,  6.90s/it]\n","100%|██████████| 10/10 [01:05<00:00,  6.58s/it]\n","100%|██████████| 10/10 [00:35<00:00,  3.53s/it]\n"]}]},{"cell_type":"code","source":["vis = pyLDAvis.prepare(btm.phi_wz.T, topics, np.count_nonzero(X, axis=1), vocab, np.sum(X, axis=0))\n","pyLDAvis.save_html(vis,'simple_btm.html')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BR4oiDBednc6","executionInfo":{"status":"ok","timestamp":1668397934359,"user_tz":480,"elapsed":3218,"user":{"displayName":"Swaptik Chowdhury","userId":"10959842892116898495"}},"outputId":"55f1163f-177d-451a-c78c-03516c32a6d6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/pyLDAvis/_prepare.py:247: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n","  by='saliency', ascending=False).head(R).drop('saliency', 1)\n"]}]},{"cell_type":"code","source":["topic_summuary(btm.phi_wz.T, X, vocab, 10)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yKfGmzOfz14i","executionInfo":{"status":"ok","timestamp":1668397934727,"user_tz":480,"elapsed":378,"user":{"displayName":"Swaptik Chowdhury","userId":"10959842892116898495"}},"outputId":"ed1d8ec1-c13f-4b6d-9159-31f5456b8089"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Topic 0 | Coherence=-130.81 | Top words= web3 web nft like join people check crypto new community\n","Topic 1 | Coherence=-135.41 | Top words= web3 web nft time space nfts blockchain people today metaverse\n","Topic 2 | Coherence=-139.04 | Top words= web3 web blockchain new community join crypto space metaverse users\n","Topic 3 | Coherence=-138.15 | Top words= web3 web 2022 million nft game games funding platform metaverse\n","Topic 4 | Coherence=-123.11 | Top words= web3 new world community digital nft web nfts join connect\n"]},{"output_type":"execute_result","data":{"text/plain":["{'coherence': [-130.80897775921983,\n","  -135.40775126892092,\n","  -139.03812562818015,\n","  -138.15190510432294,\n","  -123.10528524598128],\n"," 'top_words': [array(['web3', 'web', 'nft', 'like', 'join', 'people', 'check', 'crypto',\n","         'new', 'community'], dtype='<U132'),\n","  array(['web3', 'web', 'nft', 'time', 'space', 'nfts', 'blockchain',\n","         'people', 'today', 'metaverse'], dtype='<U132'),\n","  array(['web3', 'web', 'blockchain', 'new', 'community', 'join', 'crypto',\n","         'space', 'metaverse', 'users'], dtype='<U132'),\n","  array(['web3', 'web', '2022', 'million', 'nft', 'game', 'games',\n","         'funding', 'platform', 'metaverse'], dtype='<U132'),\n","  array(['web3', 'new', 'world', 'community', 'digital', 'nft', 'web',\n","         'nfts', 'join', 'connect'], dtype='<U132')]}"]},"metadata":{},"execution_count":17}]}]}